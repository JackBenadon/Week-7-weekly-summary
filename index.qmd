---
title: "Week 7 Weekly Summary"
author: "Author Name"
title-block-banner: true
title-block-style: default
toc: true
format: html
# format: pdf
---

---

## Tuesday, Feb 21

::: {.callout-important}
## TIL

Today, I learnt the following concepts in class:

1. Multicolinearity 
1. Variable selection
1. Shrinkage estimators
:::


```{R results='hide'}
#| output: false
library(dplyr)
library(purrr)
library(ISLR2)
library(tidyr)
library(readr)
library(glmnet)
library(caret)
library(car)
```

## Regularization / Shrinkage Estimators
Regularization achieves a simialr objective using a slightly different strategy.

Mathematically we're trying to find a solution to this:
$$
(b1, b2, \dots, b_p) = arg min L(\beta_0, \beta_2, \dots, \beta_p)
$$
If we want to select only a subset of these variables in our final model we can include a penalty term:
$p_\lambda(\beta_1, \dots, \beta_p)$
This favors solutions which select smaller subsets of the variables.

The most common penalty functions are:

1. ridge regression estimator. 

2. LASSO regression. It is the most popular one, it looks at the absolute value of the estimators

3. general case in `glmnet()`

unlike lm glmnet doesnt take in a formula

glmnet(X,y)
X: matrix of the covariates
y: response vector

```{R}
#attach the work from last week
library(ISLR2)
attach(Boston)
df <- Boston


full_model <- lm(medv ~ . , df)


model_indus <- lm(medv ~indus, df)

R <- df %>%
keep(is.numeric) %>%
cor()

new_cols <- colnames(df) [-c(5,13)]
model <-lm(medv ~ . , df %>% select(-c(indus, nox, dis)))


```

```{R}
X <- model.matrix(full_model)[, -1]
head(X)
```
```{R}
all_cols <- 1:ncol(X)
drop_scale <- c(4)
include_scale <- all_cols[-drop_scale]

for (i in include_scale){ X[, i] <- scale (X[, i])}
```

```{R}
y <- df$medv
```
```{R}
library(glmnet)
lasso <- cv.glmnet(X,y,alpha =1)

plot(lasso)
```
The graph above is plotting the sum of squares residuals (mean square error) on the y and Log($\lambda$) on the x axis. As lambda increases the effect of the penalty on the final solution will get a lot stronger.

### _How do I know what the appropriate value of $\lambda$ is?_
Minimizing the mean square error is one way to approach it but it would pick a lot of variables, so this isn't an effective way to solve this. 
The dotted line on the graph is called the `elbow point`. They mark the most stable solutions before the graph spikes upwards and the MSE increases too much.

```{R}
lambdas <- 10^ seq(-2, 1, length.out = 1000)
lasso <- cv.glmnet(X, y ,alpha =1, lambda = lambdas)
plot(lasso)
```

```{R}
lasso_coef <- coef(lasso, s = 'lambda.min')
selected_vars <- rownames(lasso_coef)[which(abs(lasso_coef)> 0)] [-1]
lasso_coef
selected_vars
```
lasso is very useful because you can do it in one step. However, this can be deceiving as there are steps you have to do prior.


## Gradient Descent
#### _A general recipe for fitting models_
A minimizer has to have a flat line and that the second derivative has to be positive (This helps confirm that the point is a minimizer rather than a maximizer).

1. compute the gradient

2. Choose a step size

3. Perform gradient descent

4. stop when the relative improvement is small

#### Using gradient descent with the `cars` dataset

```{R}
attach(cars)
t(cars)
```

```{R}
plot(cars)
```

```{R}
Loss <- function(b, x,y){
  squares <- (y - b[1]-b[2] * x)^2
  return(sum(squares))
}
b <- rnorm(2)
Loss(b, cars$speed, cars$dist)
```

```{R}
grad <- function(b, Loss, x, y, eps=1e-5) {
  b0_up <- Loss(c(b[1] + eps, b[2]) ,x,y)
  b0_dn <- Loss(c(b[1] - eps, b[2]) ,x,y)
  
  b1_up <- Loss(c(b[1] + eps, b[2]) ,x,y)
  b1_dn <- Loss(c(b[1] - eps, b[2]) ,x,y)
  
  grad_b0_L <- (b0_up - b0_dn) / (2*eps)
  grad_b1_L <- (b1_up - b1_dn) / (2*eps)
  
  return(c(grad_b0_L, grad_b1_L))
}

grad(b,Loss, cars$speed, cars$dist)
```

```{R}
steps <- 1000
L <- rep(Inf, steps)
eta <- 1e-7
b <- 10 *rnorm(2)


for (i in 1:steps){
  b <- b - eta * grad(b,Loss, cars$speed, cars$dist)
  L[i] <- Loss(b, cars$speed, cars$dist)
}
```



## Thursday, Feb 23



::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Automatic Differentiation
1. pytorch
1. Cross validation
:::

```{R}
library(torch)
```
Torch was developed by Facebook and we will use it for the more advanced machine learning examples in this class.

## Automatic Differentiation
The goal of automatic differentiation is to get rid of having to write functions like the `grad` function we wrote last class. Ideally we automatically compute the gradient with respect to the parameters.

```{R}
x <- torch_randn(c(5,1), requires_grad = TRUE)
x
```
In torch everything is called a _tensor_. For example a vector is a 1 dimensional tensor.

```{R}
f <- function(x){
    torch_norm(x)^10
}

y <- f(x)
y
y$backward()
```

```{R}
x$grad
```

```{R}
(5 * torch_norm(x)^8) * (2*x)
```
```{R}
f <- function(x,y){
    sum(x*y)
}
z <- f(x,y)
z #z is the dot product of x and y

```

```{R}
c(x$grad, y$grad) #x$grad = partial derivative of x 
```
#### Example 3:
```{R}
x<- torch_tensor(cars$speed, dtype = torch_float())
y <- torch_tensor(cars$dist, dtype = torch_float())

plot(x,y)
```

```{R}
b <- torch_zeros(c(2,1), dtype = torch_float(), requires_grad = TRUE)
b
```

```{R}
loss <- nn_mse_loss()
```

```{R}
b <- torch_zeros(c(2,1), dtype = torch_float(), requires_grad = TRUE)
steps <- 10000
L <- rep(Inf,steps)
eta <- 0.5
optimizer <- optim_adam(b, lr=eta) #optional but gradient descent isnt optimal, there other more efficient methods. Torch has multiple and they will change your result.
for (i in 1:steps){
  y_hat <- x*b[2] +b[1]
  l <- loss(y_hat, y)
  
  L[i] <- l$item()
  optimizer$zero_grad()
  l$backward()
  optimizer$step()
  
  if (i %in% c(1:10) || i %% 200 == 0){
      cat(sprintf("Iteration: %s\t Loss value: %s\n", i, L[i]))
  }
}
```

```{R}
plot(x,y)
```
#### Short tangent on AIC
There isn't much that is special about it but it is something that people like to use. However, data miners and data scientists started using an other model called cross validation


## Cross validation
We split the data set in 2 parts, randomly chooses 100 rows and not let the model see then. We use the 900 row to train it and the 100 to test it.
```{R}
df <- Boston %>% drop_na()
head(df)
dim(df)
```

```{R}
k <- 5
fold <- sample(1:nrow(df), nrow(df)/5)
fold
```

```{R}
train <- df %>% slice(-fold)
test <- df %>% slice(fold)

```

```{R}
nrow(test) + nrow(train) - nrow(df)
```

```{R}
model<-lm(medv ~ ., data = train)
summary(model)
```

```{R}
y_test <- predict(model, newdata = test)
```
```{R}
mspe <- mean((test$medv - y_test)^2)
mspe
```

## K-Fold Cross Validation
Now instead of splitting it as 1/5 we can split it however we want. We can give the model as much training and testing data as we see fit. When we train it as 80% train and 20% test the mspe went down. This is because there is variability in the result that comes from what data we split. Since we split it randomly the result changes.

K-Fold cross validation eliminates this by splitting the data set into 5 disjoint subsets. And then we select one of the 5 as the test and the other 4 as training data sets. 










[^footnote]: You can include some footnotes here